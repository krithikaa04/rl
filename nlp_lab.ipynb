{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "tmjUCIRN62_A",
        "2vQcBOXd7Gts",
        "tgxULj0sFQYE",
        "S6FOjBVkGog4",
        "nWT6bATwHRZy",
        "d5uptQi9FVvi",
        "iOFOovV5FY3f"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **CYK and PCYK**"
      ],
      "metadata": {
        "id": "njmF3QG755PI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CYK and PCYK - Deeksha"
      ],
      "metadata": {
        "id": "tmjUCIRN62_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from nltk import Tree\n",
        "import nltk\n",
        "\n",
        "# Ensure nltk resources are available\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "qEHVd3jH5UTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# CYK PARSER (Non-Probabilistic)\n",
        "# ---------------------------\n",
        "def cyk_parser(grammar, tokens):\n",
        "    n = len(tokens)\n",
        "    table = [[set() for _ in range(n)] for _ in range(n)]\n",
        "    back = [[defaultdict(list) for _ in range(n)] for _ in range(n)]\n",
        "\n",
        "    # Inverse grammar mapping: RHS -> LHS\n",
        "    rhs_to_lhs = defaultdict(set)\n",
        "    for lhs, rules in grammar.items():\n",
        "        for rhs in rules:\n",
        "            rhs_to_lhs[tuple(rhs)].add(lhs)\n",
        "\n",
        "    # Fill diagonals\n",
        "    for i, token in enumerate(tokens):\n",
        "        for lhs in rhs_to_lhs.get((token,), []):\n",
        "            table[i][i].add(lhs)\n",
        "\n",
        "    # Fill upper triangle\n",
        "    for l in range(2, n+1):\n",
        "        for i in range(n - l + 1):\n",
        "            j = i + l - 1\n",
        "            for k in range(i, j):\n",
        "                for B in table[i][k]:\n",
        "                    for C in table[k+1][j]:\n",
        "                        for A in rhs_to_lhs.get((B, C), []):\n",
        "                            table[i][j].add(A)\n",
        "                            back[i][j][A].append((k, B, C))\n",
        "\n",
        "    # Recursive tree building\n",
        "    def build_tree(i, j, symbol):\n",
        "        if i == j:\n",
        "            return (symbol, tokens[i])\n",
        "        for k, B, C in back[i][j].get(symbol, []):\n",
        "            left = build_tree(i, k, B)\n",
        "            right = build_tree(k+1, j, C)\n",
        "            return (symbol, left, right)\n",
        "\n",
        "    if 'S' in table[0][n-1]:\n",
        "        return build_tree(0, n-1, 'S')\n",
        "    else:\n",
        "        return None"
      ],
      "metadata": {
        "id": "Yd0Wt-UT6rh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# EXAMPLE USAGE\n",
        "# ---------------------------\n",
        "tokens = ['the', 'cat', 'chased', 'the', 'mouse']\n",
        "\n",
        "# CYK grammar\n",
        "grammar = {\n",
        "    'S': [['NP', 'VP']],\n",
        "    'VP': [['V', 'NP']],\n",
        "    'NP': [['Det', 'N']],\n",
        "    'Det': [['the']],\n",
        "    'N': [['cat'], ['mouse']],\n",
        "    'V': [['chased']]\n",
        "}\n",
        "\n",
        "# Run CYK\n",
        "cyk_tree = cyk_parser(grammar, tokens)\n",
        "if cyk_tree:\n",
        "    print(\"CYK Parse Tree (tuple):\")\n",
        "    print(cyk_tree)\n",
        "    nltk_tree = tuple_to_nltk_tree(cyk_tree)\n",
        "    nltk_tree.pretty_print()\n",
        "else:\n",
        "    print(\"No parse tree found.Invalid input sentence\")"
      ],
      "metadata": {
        "id": "C2BIy3LX6yWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CYK and PCYK - KV"
      ],
      "metadata": {
        "id": "2vQcBOXd7Gts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "from nltk import Tree\n",
        "\n",
        "# --- Extract rules from trees ---\n",
        "def extract_rules_from_tree(tree):\n",
        "    rules = []\n",
        "    if len(tree) == 2 and isinstance(tree[1], str):  # Terminal\n",
        "        lhs, word = tree\n",
        "        rules.append((lhs, (word,)))\n",
        "    elif len(tree) == 3:\n",
        "        lhs, left, right = tree\n",
        "        rules.append((lhs, (left[0], right[0])))\n",
        "        rules += extract_rules_from_tree(left)\n",
        "        rules += extract_rules_from_tree(right)\n",
        "    return rules\n",
        "\n",
        "# --- Learn rule probabilities ---\n",
        "def compute_rule_probabilities(trees):\n",
        "    rule_counts = defaultdict(int)\n",
        "    lhs_counts = defaultdict(int)\n",
        "\n",
        "    for tree in trees:\n",
        "        rules = extract_rules_from_tree(tree)\n",
        "        for lhs, rhs in rules:\n",
        "            rule_counts[(lhs, rhs)] += 1\n",
        "            lhs_counts[lhs] += 1\n",
        "\n",
        "    rule_probs = {\n",
        "        (lhs, rhs): count / lhs_counts[lhs]\n",
        "        for (lhs, rhs), count in rule_counts.items()\n",
        "    }\n",
        "\n",
        "    return rule_probs\n",
        "\n",
        "# --- Probabilistic CYK Parser ---\n",
        "def probabilistic_cyk_parser(rule_probs, sentence):\n",
        "    n = len(sentence)\n",
        "    table = [[defaultdict(lambda: (-math.inf, None)) for _ in range(n)] for _ in range(n)]\n",
        "\n",
        "    # Fill in terminals\n",
        "    for i in range(n):\n",
        "        word = sentence[i]\n",
        "        for (lhs, rhs), prob in rule_probs.items():\n",
        "            if rhs == (word,):\n",
        "                table[i][i][lhs] = (math.log(prob), (word,))\n",
        "\n",
        "    # Fill in non-terminals\n",
        "    for span in range(2, n+1):  # span length\n",
        "        for i in range(n - span + 1):\n",
        "            j = i + span - 1\n",
        "            for k in range(i, j):\n",
        "                for (lhs, rhs), prob in rule_probs.items():\n",
        "                    if len(rhs) == 2:\n",
        "                        B, C = rhs\n",
        "                        prob_B, back_B = table[i][k].get(B, (-math.inf, None))\n",
        "                        prob_C, back_C = table[k+1][j].get(C, (-math.inf, None))\n",
        "                        if prob_B > -math.inf and prob_C > -math.inf:\n",
        "                            total_prob = math.log(prob) + prob_B + prob_C\n",
        "                            if total_prob > table[i][j][lhs][0]:\n",
        "                                table[i][j][lhs] = (total_prob, (B, C, i, k, k+1, j))\n",
        "\n",
        "    # Check if S can generate the sentence\n",
        "    if 'S' not in table[0][n-1]:\n",
        "        return None, 0.0\n",
        "\n",
        "    # Build the best tree\n",
        "    def build_tree(sym, i, j):\n",
        "        prob, back = table[i][j][sym]\n",
        "        if len(back) == 1:\n",
        "            return (sym, back[0])  # Terminal\n",
        "        else:\n",
        "            B, C, i1, k1, k2, j2 = back\n",
        "            return (sym, build_tree(B, i1, k1), build_tree(C, k2, j2))\n",
        "\n",
        "    best_tree = build_tree('S', 0, n-1)\n",
        "    best_log_prob = table[0][n-1]['S'][0]\n",
        "    return best_tree, math.exp(best_log_prob)\n",
        "\n",
        "# --- Example usage ---\n",
        "training_trees = [\n",
        "    ('S', ('NP', 'she'), ('VP', ('V', 'eats'), ('NP', 'fish'))),\n",
        "    ('S', ('NP', 'fish'), ('VP', 'eats')),\n",
        "    ('S', ('NP', 'she'), ('VP', 'eats')),\n",
        "    ('S', ('NP', 'fish'), ('VP', ('V', 'eats'), ('NP', 'fish')))\n",
        "]\n",
        "\n",
        "# Learn rule probabilities\n",
        "rule_probs = compute_rule_probabilities(training_trees)\n",
        "\n",
        "# Input sentence\n",
        "sentence = ['she', 'eats', 'fish']\n",
        "\n",
        "# Parse and get best tree\n",
        "best_tree, tree_prob = probabilistic_cyk_parser(rule_probs, sentence)\n",
        "\n",
        "# --- Output ---\n",
        "print(\"Best Parse Tree:\")\n",
        "print(best_tree)\n",
        "print(f\"\\nProbability: {tree_prob:.6f}\")\n",
        "\n",
        "print(\"\\nLearned Grammar Rules with Probabilities:\")\n",
        "for (lhs, rhs), p in rule_probs.items():\n",
        "    print(f\"{lhs} -> {' '.join(rhs)} [{p:.3f}]\")"
      ],
      "metadata": {
        "id": "_LY3damu7ian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# CONVERT TO nltk.Tree & DISPLAY\n",
        "# ---------------------------\n",
        "def tuple_to_nltk_tree(tree_tuple):\n",
        "    if isinstance(tree_tuple, tuple):\n",
        "        label = tree_tuple[0]\n",
        "        children = [tuple_to_nltk_tree(child) for child in tree_tuple[1:]]\n",
        "        return Tree(label, children)\n",
        "    else:\n",
        "        return tree_tuple\n",
        "\n",
        "if best_tree:\n",
        "    print(\"\\nPCYK Parse Tree (tuple)- returns the most probable parse tree:\")\n",
        "    print(best_tree)\n",
        "    nltk_tree = tuple_to_nltk_tree(best_tree)\n",
        "    nltk_tree.pretty_print()\n",
        "else:\n",
        "    print(\"No parse tree found.Invalid input sentence\")"
      ],
      "metadata": {
        "id": "ctZD0M6w5DpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Seq2Seq - use this**"
      ],
      "metadata": {
        "id": "3_ylPRMDDUlc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/seq2seq.csv\",encoding=\"latin-1\")\n",
        "pairs = list(zip(df[\"Source\"], df[\"Target\"]))\n",
        "\n",
        "pairs = []\n",
        "with open(\"translations.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        parts = line.strip().split('\\t')\n",
        "        if len(parts) == 2:\n",
        "            pairs.append((parts[0], parts[1]))\n",
        "\n",
        "# parts = line.strip().split(',')\n",
        "# parts = line.strip().split('|')"
      ],
      "metadata": {
        "id": "XTXPmn7WbPKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "# Dummy dataset\n",
        "pairs = [\n",
        "    (\"i am a student\", \"je suis un Ã©tudiant\"),\n",
        "    (\"he is a teacher\", \"il est un professeur\"),\n",
        "    (\"she is happy\", \"elle est heureuse\"),\n",
        "    (\"they are playing\", \"ils jouent\"),\n",
        "    (\"you are smart\", \"tu es intelligent\")\n",
        "]\n",
        "\n",
        "# Tokenize and build vocab\n",
        "def tokenize(sentence):\n",
        "    return sentence.lower().split()\n",
        "\n",
        "def build_vocab(sentences):\n",
        "    vocab = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2}\n",
        "    idx = 3\n",
        "    for sent in sentences:\n",
        "        for word in tokenize(sent):\n",
        "            if word not in vocab:\n",
        "                vocab[word] = idx\n",
        "                idx += 1\n",
        "    return vocab\n",
        "\n",
        "src_vocab = build_vocab([src for src, tgt in pairs])\n",
        "tgt_vocab = build_vocab([tgt for src, tgt in pairs])\n",
        "inv_tgt_vocab = {v: k for k, v in tgt_vocab.items()}\n",
        "\n",
        "def encode(sentence, vocab):\n",
        "    return [vocab[\"<sos>\"]] + [vocab[word] for word in tokenize(sentence)] + [vocab[\"<eos>\"]]\n",
        "\n",
        "data = [(encode(src, src_vocab), encode(tgt, tgt_vocab)) for src, tgt in pairs]\n",
        "\n",
        "SRC_VOCAB_SIZE = len(src_vocab)\n",
        "TGT_VOCAB_SIZE = len(tgt_vocab)\n",
        "EMBED_SIZE = 32\n",
        "HIDDEN_SIZE = 64\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, embed_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim)    # IF U R USING RNN ---> self.rnn = nn.RNN(emb_dim, hid_dim)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.embedding(src)\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        return hidden, cell\n",
        "\n",
        "        '''# RNN version\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "        return hidden, None'''\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, embed_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim)    # IF U R USING RNN ----> self.rnn = nn.RNN(emb_dim, hid_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        input = input.unsqueeze(0)\n",
        "        embedded = self.embedding(input)\n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        prediction = self.fc(output.squeeze(0))\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "        ''' # RNN version\n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        return prediction, hidden, None'''\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
        "        tgt_len = tgt.shape[0]\n",
        "        batch_size = 1\n",
        "        tgt_vocab_size = self.decoder.fc.out_features\n",
        "\n",
        "        outputs = torch.zeros(tgt_len, tgt_vocab_size)\n",
        "        hidden, cell = self.encoder(src)\n",
        "\n",
        "        input = tgt[0]  # <sos>\n",
        "        for t in range(1, tgt_len):\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            outputs[t] = output\n",
        "            top1 = output.argmax(1)\n",
        "            input = tgt[t] if random.random() < teacher_forcing_ratio else top1\n",
        "        return outputs\n",
        "\n",
        "encoder = Encoder(SRC_VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE)\n",
        "decoder = Decoder(TGT_VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE)\n",
        "model = Seq2Seq(encoder, decoder)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(100):\n",
        "    total_loss = 0\n",
        "    for src, tgt in data:\n",
        "        src_tensor = torch.tensor(src).unsqueeze(1)  # (seq_len, 1)\n",
        "        tgt_tensor = torch.tensor(tgt).unsqueeze(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src_tensor, tgt_tensor)\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        tgt_tensor = tgt_tensor[1:].view(-1)\n",
        "\n",
        "        loss = criterion(output, tgt_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "# --------------------------\n",
        "# Inference\n",
        "# --------------------------\n",
        "def translate(model, sentence, max_len=10):\n",
        "    model.eval()\n",
        "    tokens = encode(sentence, src_vocab)\n",
        "    src_tensor = torch.tensor(tokens).unsqueeze(1)\n",
        "\n",
        "    hidden, cell = model.encoder(src_tensor)\n",
        "    input = torch.tensor([tgt_vocab[\"<sos>\"]])\n",
        "\n",
        "    result = []\n",
        "    for _ in range(max_len):\n",
        "        output, hidden, cell = model.decoder(input, hidden, cell)\n",
        "        top1 = output.argmax(1).item()\n",
        "        if top1 == tgt_vocab[\"<eos>\"]:\n",
        "            break\n",
        "        result.append(inv_tgt_vocab[top1])\n",
        "        input = torch.tensor([top1])\n",
        "\n",
        "    return ' '.join(result)\n",
        "\n",
        "\n",
        "# Test translation\n",
        "print(\"\\nTranslation Examples:\")\n",
        "print(\"Input: 'i am happy'\")\n",
        "print(\"Output:\", translate(model, \"i am happy\"))\n",
        "\n",
        "print(\"Input: 'you are smart'\")\n",
        "print(\"Output:\", translate(model, \"you are smart\"))"
      ],
      "metadata": {
        "id": "1q9dauh-DZ9B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0f73e57-1d78-4871-8ecc-13559391ad2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 14.1848\n",
            "Epoch 10, Loss: 10.2953\n",
            "Epoch 20, Loss: 4.6576\n",
            "Epoch 30, Loss: 2.0354\n",
            "Epoch 40, Loss: 1.0060\n",
            "Epoch 50, Loss: 0.5856\n",
            "Epoch 60, Loss: 0.3863\n",
            "Epoch 70, Loss: 0.2777\n",
            "Epoch 80, Loss: 0.2113\n",
            "Epoch 90, Loss: 0.1674\n",
            "\n",
            "Translation Examples:\n",
            "Input: 'i am happy'\n",
            "Output: je suis un Ã©tudiant\n",
            "Input: 'you are smart'\n",
            "Output: tu es intelligent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformers**"
      ],
      "metadata": {
        "id": "leGfudFsFLfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Translation - Deeksha"
      ],
      "metadata": {
        "id": "tgxULj0sFQYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install sacremoses\n",
        "!pip install evaluate\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    MarianTokenizer,\n",
        "    MarianMTModel,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "\n",
        "# Config\n",
        "model_name = \"Helsinki-NLP/opus-mt-en-fr\"\n",
        "batch_size = 4\n",
        "epochs = 3\n",
        "\n",
        "# Load dataset (CSV with 'src' and 'tgt' columns)\n",
        "df = pd.read_csv(\"translation.txt\",sep=\"\\t\")  # Your dataset here\n",
        "print(df)\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "model = MarianMTModel.from_pretrained(model_name)\n",
        "\n",
        "# Preprocessing\n",
        "def preprocess(example):\n",
        "    model_inputs = tokenizer(example[\"source\"], max_length=128, truncation=True, padding=\"max_length\")\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(example[\"target\"], max_length=128, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(preprocess, batched=True)\n",
        "\n",
        "# Training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./finetuned-en-fr\",\n",
        "    evaluation_strategy=\"no\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    num_train_epochs=epochs,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=1,\n",
        "    logging_dir='./logs',\n",
        "    report_to=\"none\"  # Disable WandB, Comet, etc.\n",
        ")\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "# Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Save model\n",
        "model.save_pretrained(\"finetuned-marian-en-fr\")\n",
        "tokenizer.save_pretrained(\"finetuned-marian-en-fr\")\n",
        "\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "def translate(text, model_dir=\"finetuned-marian-en-fr\"):\n",
        "    tokenizer = MarianTokenizer.from_pretrained(model_dir)\n",
        "    model = MarianMTModel.from_pretrained(model_dir)\n",
        "    encoded = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    generated = model.generate(**encoded)\n",
        "    return tokenizer.decode(generated[0], skip_special_tokens=True)\n",
        "\n",
        "# Example\n",
        "print(translate(\"Hi, I am deeksha\"))\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import evaluate\n",
        "\n",
        "# Load BLEU metric\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "\n",
        "# Example list of predictions and references\n",
        "predictions = [\"Je t'aime beaucoup.\"]\n",
        "references = [[\"Je vous aime beaucoup.\"]]  # note: reference must be a list of lists\n",
        "\n",
        "# Compute BLEU\n",
        "results = bleu.compute(predictions=predictions, references=references)\n",
        "print(f\"BLEU score: {results['bleu']:.4f}\")"
      ],
      "metadata": {
        "id": "iA6gruS2FVXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Translation - Prethika"
      ],
      "metadata": {
        "id": "S6FOjBVkGog4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import MarianMTModel, MarianTokenizer, Trainer, TrainingArguments\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk import word_tokenize, download\n",
        "from datasets import Dataset\n",
        "import evaluate\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "download('punkt')\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"seq2seq.csv\", encoding='latin-1')\n",
        "\n",
        "# Convert to HuggingFace Dataset\n",
        "dataset = Dataset.from_pandas(df.rename(columns={\"Source\": \"src\", \"Target\": \"tgt\"}))\n",
        "\n",
        "# Load model & tokenizer\n",
        "model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "model = MarianMTModel.from_pretrained(model_name)\n",
        "\n",
        "# Tokenization\n",
        "def preprocess_data(batch):\n",
        "    source = tokenizer(batch[\"src\"], max_length=128, padding=\"max_length\", truncation=True)\n",
        "    target = tokenizer(batch[\"tgt\"], max_length=128, padding=\"max_length\", truncation=True)\n",
        "    source[\"labels\"] = target[\"input_ids\"]\n",
        "    return source\n",
        "\n",
        "dataset = dataset.map(preprocess_data, batched=True)\n",
        "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "# Training args\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"output\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Translate example and evaluate\n",
        "def translate(text):\n",
        "    tokens = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    translated = model.generate(**tokens, max_length=100)\n",
        "    return tokenizer.decode(translated[0], skip_special_tokens=True)\n",
        "\n",
        "def evaluate_translation(predicted, reference):\n",
        "    print(\"\\nðŸ”¹ Predicted Translation:\\n\", predicted)\n",
        "    print(\"ðŸ”¸ Reference Translation:\\n\", reference)\n",
        "\n",
        "    bleu = sentence_bleu([word_tokenize(reference)], word_tokenize(predicted))\n",
        "    print(f\"\\nðŸŸ¢ BLEU Score: {bleu:.4f}\")\n",
        "\n",
        "    rouge = evaluate.load(\"rouge\")\n",
        "    scores = rouge.compute(predictions=[predicted], references=[reference])\n",
        "\n",
        "    print(\"\\nðŸŸ¢ ROUGE Scores:\")\n",
        "    for k, v in scores.items():\n",
        "        print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "    plt.bar(scores.keys(), scores.values(), color=\"skyblue\")\n",
        "    plt.title(\"ROUGE Scores - Machine Translation\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.show()\n",
        "\n",
        "translated_text = translate(\"Hello, how are you?\")\n",
        "evaluate_translation(translated_text, \"Hola, cÃ³mo estÃ¡s?\")"
      ],
      "metadata": {
        "id": "R7t2ir1VGo4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Sumamrization - Deeksha\n"
      ],
      "metadata": {
        "id": "nWT6bATwHRZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer, TrainingArguments, Trainer\n",
        "\n",
        "# Load a smaller dataset sample\n",
        "data = pd.read_csv(\"summary.txt\",sep = '\\t')\n",
        "sample_size = min(100, len(data))\n",
        "data = data.sample(n=sample_size, random_state=42)\n",
        "dataset = Dataset.from_pandas(data)\n",
        "\n",
        "# Use smaller DistilBART model\n",
        "model_name = \"sshleifer/distilbart-cnn-12-6\"\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Preprocess\n",
        "def preprocess_function(examples):\n",
        "    inputs = tokenizer(examples[\"text\"], max_length=512, truncation=True, padding=\"max_length\")\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"summary\"], max_length=128, truncation=True, padding=\"max_length\")\n",
        "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Training arguments (CPU optimized)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./finetuned_bart_summary_cpu\",\n",
        "    evaluation_strategy=\"no\",  # skip eval\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=2,  # keep low for CPU\n",
        "    num_train_epochs=3,  # 1 epoch for testing\n",
        "    logging_strategy=\"no\",\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"none\"  # no wandb or hub\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Train (fast on CPU)\n",
        "trainer.train()\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained(\"finetuned-bart-summary-cpu\")\n",
        "tokenizer.save_pretrained(\"finetuned-bart-summary-cpu\")\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "def summarize(text, model_path=\"finetuned-bart-summary-cpu\"):\n",
        "    summarization_model = pipeline(\n",
        "        \"summarization\",\n",
        "        model=model_path,\n",
        "        tokenizer=model_path\n",
        "    )\n",
        "\n",
        "    # Generate the summary\n",
        "    summary = summarization_model(text)[0]['summary_text']\n",
        "    return summary\n",
        "\n",
        "# Example\n",
        "text = '''Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions.\n",
        "          Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance.\n",
        "          ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine.The application of ML to business problems is known as predictive analytics.\n",
        "          Statistics and mathematical optimization (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning.\n",
        "          From a theoretical viewpoint, probably approximately correct learning provides a framework for describing machine learning.'''\n",
        "\n",
        "print(summarize(text))"
      ],
      "metadata": {
        "id": "flYfTOJaHUMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Sumamrization - Prethika"
      ],
      "metadata": {
        "id": "d5uptQi9FVvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import nltk\n",
        "from datasets import Dataset\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import evaluate\n",
        "import matplotlib.pyplot as plt\n",
        "#pip install rouge_score\n",
        "#pip install evaluate\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv(\"dummy_summarization_data.csv\")\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Convert pandas to HF dataset\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_function(examples):\n",
        "    inputs = [f\"summarize: {text}\" for text in examples[\"article\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(examples[\"summary\"], max_length=64, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Tokenize and split dataset\n",
        "dataset = dataset.map(preprocess_function, batched=True)\n",
        "dataset = dataset.train_test_split(test_size=0.3, seed=42)\n",
        "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./output\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Evaluation\n",
        "sample_input = df.iloc[0][\"article\"]\n",
        "input_ids = tokenizer(f\"summarize: {sample_input}\", return_tensors=\"pt\", max_length=512, truncation=True).input_ids\n",
        "output_ids = model.generate(input_ids, max_length=64, num_beams=4, early_stopping=True)\n",
        "summary_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"ðŸ”¹Generated Summary:\\n\", summary_text)\n",
        "print(\"ðŸ”¸Reference Summary:\\n\", df.iloc[0][\"summary\"])\n",
        "\n",
        "# BLEU Score\n",
        "bleu = sentence_bleu([nltk.word_tokenize(df.iloc[0][\"summary\"])], nltk.word_tokenize(summary_text))\n",
        "print(f\"\\nðŸŸ¢ BLEU Score: {bleu:.4f}\")\n",
        "\n",
        "# ROUGE Score\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "results = rouge.compute(predictions=[summary_text], references=[df.iloc[0][\"summary\"]])\n",
        "\n",
        "for k, v in results.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "plt.bar(results.keys(), results.values(), color=\"skyblue\")\n",
        "plt.title(\"ROUGE Scores\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.ylim(0, 1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EEDCMfPBFYcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis"
      ],
      "metadata": {
        "id": "iOFOovV5FY3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets scikit-learn\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    DistilBertTokenizerFast,\n",
        "    DistilBertForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "\n",
        "# Load and prepare dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/SEMESTER 8/Natural Language Processing/Data/sentiment_data.csv\")  # Replace with your file path\n",
        "label_map = {\"negative\": 0, \"positive\": 1}\n",
        "df[\"label\"] = df[\"label\"].map(label_map)\n",
        "\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "# Split into train and test\n",
        "dataset = dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "# Load tokenizer and model\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
        "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "# Preprocessing\n",
        "def preprocess(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
        "\n",
        "tokenized = dataset.map(preprocess, batched=True)\n",
        "\n",
        "# Minimal training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./tmp\",  # Required even if not saving\n",
        "    per_device_train_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized[\"train\"],\n",
        "    eval_dataset=tokenized[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# Train the model (quick)\n",
        "trainer.train()\n",
        "\n",
        "# Example usage: Sentiment prediction\n",
        "def predict_sentiment(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "    predicted_class = torch.argmax(logits, dim=-1).item()\n",
        "    return \"positive\" if predicted_class == 1 else \"negative\"\n",
        "\n",
        "# Test the prediction\n",
        "example = \"I dont love this new phone!\"\n",
        "print(predict_sentiment(example))"
      ],
      "metadata": {
        "id": "-GJ9VW-oFbZX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}